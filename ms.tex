%\documentclass[10pt,iop]{emulateapj}
\documentclass[preprint]{aastex}

\usepackage{url}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{xcolor}
%\citestyle{aa}

%\bibliographystyle{apj_w_etal}

\newcommand{\etal}{{et al.\/}}
\newcommand{\Prob}{\mathtt{P}}
\newcommand{\logL}{\log\mathcal{L}}
\newcommand{\unit}[1]{\footnotesize #1}
\newcommand{\PAPER}{\mathrm{PAPER}}
\bibliographystyle{apj_w_etal}

\newcommand{\Nconf}{31}
\newcommand{\Nsrc}{32}
\definecolor{orange}{RGB}{255,127,0}

	% End definitions

%\slugcomment{DRAFT: \today}

\shorttitle{EoR time}
\shortauthors{Jacobs et al.}

\begin{document}


\title{Multi-redshift limits on the 21cm power spectrum from PAPER}
\author{
Daniel C. Jacobs\altaffilmark{1},
Aaron R. Parsons\altaffilmark{2,8},
James E. Aguirre\altaffilmark{3},
Zaki Ali\altaffilmark{2},
Judd Bowman\altaffilmark{1},
Richard F. Bradley\altaffilmark{4,5,6},
Chris L.  Carilli\altaffilmark{7,10},
David R. DeBoer\altaffilmark{8},
Matthew R. Dexter\altaffilmark{8},
Nicole E. Gugliucci\altaffilmark{5},
Pat Klima\altaffilmark{5},
Adrian Liu\altaffilmark{2,11},
Dave H. E. MacMahon\altaffilmark{8}
Jason R. Manley\altaffilmark{9},
David F. Moore\altaffilmark{3},
Jonathan C. Pober\altaffilmark{2},
Irina I. Stefan\altaffilmark{10},
William P. Walbrugh\altaffilmark{9}}

\altaffiltext{1}{School of Earth and Space Exploration, Arizona State U., Tempe, AZ}
\altaffiltext{2}{Astronomy Dept., U. California, Berkeley, CA}
\altaffiltext{3}{Dept. of Physics and Astronomy, U. Pennsylvania, Philadelphia, PA}
\altaffiltext{4}{Dept. of Electrical and Computer Engineering, U. Virginia, Charlottesville, VA}
\altaffiltext{5}{National Radio Astronomy Obs., Charlottesville, VA}
\altaffiltext{6}{Dept. of Astronomy, U. Virginia, Charlottesville, VA}
\altaffiltext{7}{National Radio Astronomy Obs., Socorro, NM}
\altaffiltext{8}{Radio Astronomy Lab., U. California, Berkeley, CA}
\altaffiltext{9}{Square Kilometer Array, South Africa Project, Cape Town, South Africa}
\altaffiltext{10}{Cavendish Lab., Cambridge, UK}
\altaffiltext{11}{Berkeley Center for Cosmological Physics, UC Berkeley, Berkeley,
CA}

\begin{abstract}
We present new observations from the Donald C. Backer Precision Array for Probing the Epoch of Reionization (PAPER) telescope which place new deeper limits on the HI power spectrum over the redshift range of $7.5<z<10.5$, extending previously published single redshift results to cover the full range accessible to the instrument. The epoch of reionization power spectrum is expected to evolve strongly with redshift, and it is this variation with cosmic history that will allow us to begin to place constraints on the physics of reionization.  The primary obstacle to this goal are bright foregrounds and the attendant systematics associated with high dynamic range measurements. Here we use filtering techniques that take advantage of the large instrumental bandwidth to make a high dynamic range subtraction of foreground power.  Power spectra at different points across the redshift range reveal the variable efficacy of this subtraction.  Noise limited measurements of $\Delta^2$ at $k=$0.2hMpc$^{-1}$ and z$=7.5$ reach as low as (48mK)$^2$ ($2\sigma$).  Most spectra demonstrate a robust foreground removal of foreground signals to the thermal noise limit, leaving systematics on the scale of the noise. To better understand the source of these residual systematics we examine different estimates of the noise in the data and compare with theoretical predictions. We find that in the case of the highest redshift power spectrum, foregrounds begin to dominate over the noise, reflecting the increased difficulty in filtering foregrounds near the edge of the instrumental band.
\end{abstract}

\keywords{reionization}


\section{Introduction}
The Epoch of Reionization, when the first luminous objects ionized the pervasive cosmological hydrogen is predicted to be observable in highly redshifted 21 cm radiation.  The Donald C. Backer Precision Array for Probing the Epoch of Reionization (PAPER, \cite{Parsons:2010p6757})\footnote{\url{eor.berkeley.edu}} is a low frequency radio interferometer experiment dedicated to opening this window on the universe.  Challenges include foregrounds which are brighter by several orders of magnitude and long integration times necessitated by the limited collecting areas of first-generation instruments. Direct observation of hydrogen before and during re-ionization is predicted to deliver a wealth of cosmological and astrophysical data, including the nature of the first stellar objects and the timing and rate of galaxy formation. Reviews on the physics of reionization as well as theoretical expectations on the nature of foregrounds may be be found in \citet{Furlanetto:2006p2267,Morales:2010p8093,Pritchard:2012p9555}.  

Other telescopes seeking to measure this signal include the Giant Metre-wave Radio Telescope (GMRT; \cite{Paciga:2013p9943}), the Low Frequency Array (LOFAR\footnote{\url{www.lofar.org}}; \cite{Yatawatta:2013p9699}) and the Murchison Widefield Array (MWA\footnote{\url{mwatelescope.org}}; \cite{Bowman:2013p9950} and \cite{Tingay:2013p9022}). 



PAPER is located in the Karoo desert at the site of the South African portion of the future Square Kilometer Array\footnote{\url{skatelescope.org}} and has doubled in size on a yearly basis since 2009, and science-grade observations have been made with each stage of the build-out.  

Here we report on deep integrations made with a 32 element array in 2011, first described in \cite{Parsons:2014p10499}, hereafter P14.  Our data reduction method was described in detail in P14, and used this pipeline to give the deepest yet limits on the HI power spectrum in the presence of bright foregrounds at redshift 7.68 using 92 nights (1800 hours) of data.  The Epoch of Reionization signal is expected to evolve strongly with redshift, and it is this variation with cosmic history that will allow us to begin to place constraints on the physics of reionization \citep{Pritchard:2008p8123,Pober:2014p10390}.  Therefore, while a detection of the 21 cm signal at even a single frequency would be a tremendous breakthrough, analysis techniques must be developed to capitalize on the wide bandwidths of the current generation of high-redshift 21cm telescopes.  Using the same data set as P14, this paper presents improved upper limits on the HI power spectrum over the redshift range $10.7>z>7.2$.  In Section  \ref{sec:observations} we summarize the observations, in Section \ref{sec:obs_meth} review the reduction methodology, we present the new upper limits in \ref{sec:results}, and in Section \ref{sec:conclusion} we offer conclusions and discussion of future work.



%
%This signal is though to be the richest data set on the sky!  Many telescopes are searching for the power spectral signature of 
%HI.  Coarsely, models of 21cm emission can be distilled into two parameters. The redshift at which point the universe was 50\% ionized ($z_i$) and the transition time from mostly neutral to mostly ionized $dz$. Thus full coverage of the spectrum is essential.


%This paper extends the result ARP2013a to cover the redshift range XXX.

\section{Observations}
\label{sec:observations}
The work here follows the same basic procedure and uses the same underlying data set as P14. Here we provide a quick summary and refer the reader to P14 for a more in-depth discussion.  A general overview of the PAPER system can be found in \cite{Parsons:2010p6757}, calibration of the primary beam in \cite{Pober:2012p8800}, and imaging results in \cite{Jacobs:2011p8438,jacobs:2013b} and \citet{Stefan:2013p9926}.  Sensitivity analysis described in \cite{Parsons:2012p9028} revealed that for the low gain elements employed by PAPER, a highly redundant ``grid'' type arrangement offers a significant sensitivity boost.  In most interferometers the locations of the antennae are optimized such that each baseline samples a different Fourier mode of the sky; this is the ideal case for reconstructing images where each mode contains different information.  For a power spectrum measurement the key metric is sensitivity per mode, rather than number of modes.  A grid configuration allows many samples of each cosmological mode, to be averaged to a high sensitivity before being combined with other Fourier modes.  The PAPER South Africa 32 antenna deployment (PSA32) was arranged in a 4$\times$8 grid with a column spacing of 30m and a row spacing of 4m.  In our analysis, as in P14, we include only the three shortest types of spacings where the reionization power is expected to be brightest. This selection includes those between adjacent columns and within at least one row of each other, a selection containing 70 $\sim$30m-long baselines.  We will use these baselines to make a one dimensional estimate of the HI power spectrum.

Observations spanning the band between 100 to 200-MHz ($13.1>z>6.1$) were recorded at a resolution of 48kHz and 10.7s  beginning Dec 7, 2011 and ending March 19, 2012 (with some down-time for maintenance) giving a total of 92 nights or 1800 hours.  Within this set we included observations in the LST range 1h - 9h where the sky dominated system temperature is at a minimum.  Note that this LST range is slightly shorter than in P14 which extended to LST of 12 hours. These last three hours were found to contribute minimally to increasing sensitivity while being dominated by bright galactic foreground emission and so have been excluded here.




\section{Reduction}
\label{sec:obs_meth}
Here we summarize our data reduction steps; for more details see Section 3 of P14.  In summary, we use 70 nearly identical baselines to make a 1D estimate along the spectral or line-of-sight direction of the reionization era HI power spectrum.  All processing, save calibration, and  the final cross-multiplication step treats each baseline as independent. Foregrounds and interference are removed on a per-baseline basis with no a-priori sky model using signal processing techniques and a physical model of the array. In the final cross-multiplication step, the last layer of systematics is estimated and removed by projecting non-physical correlations between baselines.
\subsection{Delay and Fringe rate transforms or The Fourier Furor}
\label{sec:transforms}
In several stages throughout the analysis process we take a 2D Fourier transform of the visibility spectra $V(\nu,t)$ into ``delay/fringe rate'' space where delay is the Fourier dual to frequency and similarly fringe-rate for time.  In this space, smooth spectrum sources are physically localized to delays shorter than the light travel time length of the baseline and fringe rates shorter than the angular sidereal rate perpendicular to the baseline vector. Sources at the horizon, in the direction of the baseline vector, have the longest delays, while fringe rates are highest where the celestial equator crosses the horizon. 


 In this Fourier space, sources are highly localized with deviations from a flat spectrum manifesting as a slight dispersion. The spectrum sampling function, which is uneven due to flagging of interference takes the form of a convolution by a point-spread-function (PSF) in the same way an imperfect sampling of the $uv$ plane gives rise to the angular PSF of an interferometer.  If enough data is missing this PSF can cause smooth-spectrum sources to leak beyond the horizon.  To account for this, we use a CLEAN like, iterative, peak-finder and subtraction algorithm which is limited to finding peaks within the physically allowable ranges of delay and/or fringe-rate. In this case, the 1D ``CLEAN'' beam is the Fourier transform of the spectral or time sampling function \citep{Parsons:2009p7859}.

The data analysis pipeline essentially consists of iterative application of the delay or fringe-rate transform process, with an ever tightening allowable number of modes, interleaved with stages of averaging (time, frequency, night), before finally computing a power spectrum.  This final step takes advantage of the redundant baselines to make an unbiased power spectrum estimate by cross-multiplying identical baselines and then averaging the power spectrum modes. By not combining baselines until the last step and by assuming that the line of site transform is well approximated by the delay transform we dramatically simplify the analysis procedure to a series of signal processing steps. It also obviates the need for precision sky and instrumental beam models which are required by imaging arrays.  

This isolation of foregrounds to a region below a line-of-sight $k_\parallel$ mode that increases with baseline length is also the much discussed ``wedge'' \cite{Liu:2014p10462,Liu:2014p10463,Thyagarajan:2013p10039,Pober:2013p9942,Trott:2012p10466,Morales:2012p8790,Parsons:2012p8896,Vedantham:2012p10297,Datta:2010p8781,Parsons:2009p7859}. As we only have a single length of baseline the wedge manifests as a single value of $k\sim k_\parallel$, below which foregrounds are expected to dominate.  The details of each application of the delay/fringe-rate transform will be laid out the following sections as we provide a brief walk-through of the processing pipeline. 


\subsection{Selection of Redshift Bins}
The redshift bins for which we have computed power spectra --shown in Figure \ref{fig:bins}-- have been selected from the available bandwidth using two criteria: minimizing covariance between redshifts and avoiding missing data. We minimize covariance between adjacent redshift bins by limiting overlap of each bin to the spectrum at the outer half of the bin channel range which, as will be described in section \ref{sec:power_spectrum}, is significantly down-weighted by the use of a Blackman-Harris window.   Avoiding missing data is important because the power spectrum method, developed in P14, which leverages the redundant baselines to estimate non-sky covariance and project out these contaminated modes is particularly sensitive to missing data -the inverse of the covariance is not defined- so we also select only channels that have no missing data over the entire sidereal period (see the dotted line in Figure \ref{fig:bins}).  With these constraints we arrive at the redshifts 10.3, 8.5,7.9 and 7.45.  For the purposes of comparison with P14 we also include the redshift 7.68 bin.  \citet{Dillon:2013p10497} describes a pseudo-inverse method for handling the covariance introduced by missing data. Though not implemented here, this method is in the process of being adapted for use in highly redundant analysis \citep{Liu:2014p10462,Liu:2014p10463}.  

The spectral width of the redshift bins is dictated by two competing needs. First, sensitivity and foreground reduction both benefit from wider bandwidths. On the other hand, power spectrum measurements which trace the evolution with redshift require smaller bandwidths. Often, the evolution scale is taken to be on order $\Delta z \approx 0.5$, which translates to a bandwidth limit $B<0.5 (f^2/1421)$ MHz. To balance these two competing constraints we choose a single spectral bandwidth of 20MHz, weighted by a Blackman-Harris window for an effective width of 10MHz, or $\Delta z=0.5$ at $z=7.45$ ranging up to a $\Delta z = 0.86$ at redshift 10.3.




\subsection{Initial Averaging}
  First, the raw data are down-selected to just the 70 30m long baselines described in Section \ref{sec:observations}.  %I know this isn't technically true, but I think its important that the downselect is obvious up front.
   The visibilities are then compressed in the frequency and time directions by removing delay modes and highest frequency fringe rates corresponding to a 300m baseline (the longest baseline in the array).  This filtering\footnote{See Appendix A of P14} is done in tandem with a radio frequency interference (RFI) flagging step, using the residuals which have had bright sky-like signals removed to flag 4$\sigma$ deviations before feeding the flags back into another iteration of the compression step. This mitigates the effects of bright, narrow band, interference  being scattered into higher delay modes (where reionization lives) and results in a time and frequency bin of length 39.6s and width 492.61kHz. 
  This process reduces the data volume by a factor of $\sim$20, or roughly an order of magnitude improvement on traditional time and frequency averaging which in this array would be limited to ~100kHz and 10s to avoid averaging away fringes.
  
\subsection{Calibration}
We model the gain as a per-antenna amplitude and a phase slope -physically a single time delay- and single real, low order polynomial passband for all antennae.  Because the array samples correlations redundantly, the relative calibration between antennae is numerically overdetermined and tractable as a linear algebra problem \citep{Liu:2010p10391}.  As described in P14, we compute the ratio between redundant baselines, fit for a gain and phase slope and then algebraically solve for a per-antenna solution.  Using this method we have avoided calibrating each channel independently to preserve as much frequency variation as possible.   These solutions vary little over the three month observing period, exhibiting less than 1\% r.m.s. variation. A single solution derived for for the Dec 7 data set is used for the entire observing run. Time and frequency variation of redundant solutions is explored in general in \citet{Zheng:2014p10467}.  

Relative calibration is all that is necessary to form a power spectrum. To obtain a correctly normalized power spectrum, however, it is necessary to set a flux scale for which we need a flux measurement of a calibration standard.  For this we form a beam on a known, bright calibration source.  However, by itself the redundant calibration does not contain enough phase information to phase coherently to a sky location, there remain two free phase parameters which cannot be solved by redundancy alone.   We find these by fitting a model of Pictor A, Fornax A, and the Crab Nebula during a time when the sky is dominated by these three sources while marginalizing over the unknown\footnote{Unknown in the sense of a joint uncertainty in source flux and primary beam pattern. Though the fluxes of these sources is in some cases fairly well constrained, fluxes at 150MHz are still fairly uncertain as is the PAPER beam in those directions.} apparent flux ratio between the three sources. With the delays in place we are now able form a beam on Pictor A and (for each channel) set the overall amplitude to the calibration value of 382 (f/150MHz)$^{-0.77}$Jy found in \cite{jacobs:2013b}.
  
  
 \begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/psa32_flagging_zbins.png}
\caption{\label{fig:bins} The average amount of data remaining after interference flagging over the 3 month period between Dec 2011 and March 2012 (black line) is quite high.  Redshift bins (in grey, redshift center indicated with text label) are chosen to include spectral channels with uniform weight, i.e. no missing channels while maximizing coverage over the band.  Redshift 7.68 is included for comparison with P14.  Channels with no missing data are indicated by the dotted line, visible at the edge of flagged channels. Each redshift bin is 20 MHz wide, but weighted by a Blackman-Harris window function which heavily down-weights the outer 10MHz for a Noise Equivalent Bandwidth of 10MHz.  The interference is almost exclusively dominated by two features: ORBCOMM satellites at 137MHz and an unidentified intermittent line emitter at 175MHz. The roll off at 115MHz is due the rising noise at low frequencies being incorrectly flagged as interference.  }
\end{figure} 
%The success of the remainder of the processing stages hinges on the spectral stability of the data, which can easily be assessed by comparing redundant baselines. In figure XXX we see the r.m.s. of the ratios between redundant calibrations, before and after the calibration has been applied.  
\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/psa32_trms_with_and_without_fr_filtering_z.png}
\caption{\label{fig:noise} Root mean square (rms) noise before and after integrating to 10 minute LST bins (top and bottom sets of curves) indicating the rising significance of foregrounds as we take the last deep integration step. Noise is estimated by differencing adjacent frequencies (magenta) and between redundant baselines (blue), compared with an estimate of the theoretical noise level with a $T_{sys}$ of 550K (dotted). The top three lines show noise after filtering foregrounds and binning into 40s long sidereal bins.  At this noise level the frequency and baseline differences are roughly similar, both demonstrate the same small bumps of increased noise due to interference flagging and are consistent with noise over much of the band.  The bottom three lines show the noise level after integrating up to the maximum fringe rate of 776s. The step between the top and bottom sets is the last coherent integration with noise largely decreasing by the expected factor $\sqrt{776/40}$ except in the difference between baselines (blue) which demonstrates a clear excess at all frequencies, particularly above redshift 10. With this last large jump in sensitivity we are now seeing the slight dominance of baseline covariance over the spectral derivative rms. }
\end{figure}

\subsection{Foreground Filtering and Night Averaging}
Foregrounds are filtered from the calibrated data by removing all bright delay components with light travel times less then the baseline length. Where during the previous compression step a liberal horizon of 300m (1800ns, much longer than the 30 meter baselines under study)  was used to calculate the window size, we now choose a window corresponding to the 30m baselines under study with an extra buffer of 15ns to provide a slight buffer against the 1/B$\sim$12ns resolution of the delay spectrum.  The broadband delay spectrum model is iteratively built then subtracted from the data leaving residuals which we carry into the next stage. Next, a four hour long running mean is subtracted. This removes excess correlation due to cross-talk in the analog signal chain. The residuals are then flagged once more for RFI before the 92 nights of data are averaged into 36.4 second long local sidereal time (LST) bins, which as PAPER is a drift-scanning instrument, are equivalent to bins in Right Ascension (Declination is fixed at -30\arcdeg).  During averaging we found that some LST bins were dominated by a small number of exceedingly bright samples lying well outside the rest of the gaussian distributed data. To compensate we filter the 10\% brightest samples in each bin.  The source of these outliers is not known,  a likely possibility is an instability in the analog signal chain stimulated by weather or bright interference, a circumstance that has since been observed in later seasons. 


Though the frequency and repeated nightly observations have been averaged to their maximum extent, at 40s integrations the time axis has yet to be fully exploited.  Sky-like fringes on a 30m baseline rotate much slower than 40 seconds.  Performing a final fringe-rate filter, limiting to fringe-rates expected on a 30m baseline (down from 300m in the last iteration), we arrive at a data-set averaged to 789s, the maximum possible while still maintaining fringe coherence.    The root mean square of the residual signal (seen in Figure \ref{fig:noise}) at the end of this process is close to the 3mK level expected given the total integration time and  system temperature.  

%This step is the first point at which signal loss could occur. To measure the loss we add white noise into the data at a level sufficient to double the system temperature (to make it easily detectable on the output), subtract  and then measure the fractional difference post filtration. We recover XXX\% of the injected signal.

%delay transform, covariance projection (lossless, and lossy), bin and bootstrap average,
\subsection{Power Spectrum}
\label{sec:power_spectrum}
The output of the above steps is a single calibrated and foreground filtered sidereal night of visibility data. The power spectrum is estimated in the delay spectrum of  a 10MHz bandwidth range centered on the redshift of interest. To preserve the isolation of any foregrounds which remain, we increase the spectral range by 5MHz on each side and multiply by a Blackman-Harris window thus providing a much higher dynamic range delay spectrum point spread function. 

This leaves us with 40 delay samples on each of 70 baselines which are divided into three redundant groups. Within these groups we cross correlate delay spectra between different redundant baselines.  The cross multiplication of the same delay modes between different redundant baselines provides an unbiased estimate of the power spectrum.  These "sky-like" correlations should be identical between all redundant baselines to within the level of the noise, while all other cross multiplications between delay modes should not be correlated between different baselines. In practice  these non-sky-like modes do occasionally have significant power which leaks into the correlations which sample the power spectrum.  These are removed by iteratively dividing the covariance into a model of systematics and a model of sky-like emission and then projecting out large residual modes. This is done by dividing the baselines into different groups such that all cross-multiplications are done without introducing noise bias.  For more see Appendix of C of P14.

The residual elements of the correlation matrix corresponding to cross-multiplication of matching delay bins between different baselines are all un-biased samples of the power spectrum. To estimate the final power spectrum and its uncertainty we compute the mean and variance of many random randomly-selected subsamples, sampling the dimensions: sidereal time, redundant baseline pair, and delay sign\footnote{As visibilities are complex, both the positive and negative delays  carry  information. Physically the two signs correspond to the two halves of the sky.}.



%\begin{figure}
%\centering
%\includegraphics[width=0.45\textwidth]{{figures/pspec_log_z_10.29}.png}
%\includegraphics[width=0.45\textwidth]{{figures/pspec_log_z_8.54}.png}
%\includegraphics[width=0.45\textwidth]{{figures/pspec_log_z_7.94}.png}
%\includegraphics[width=0.45\textwidth]{{figures/pspec_log_z_7.55}.png}
%\caption{\label{fig:pspecs} Left to right from top: power spectra centered on redshifts 10.3,8.5, 7.9, 7.68 and 7.55 (frequencies: 127, 149.5, 159.5 164.5, and 167 MHz.)  All with an effective bandwidth of $\pm$10MHz  covering the redshift span $\Delta z\approx$1421MHz ($B/f^2$) which ranges from 0.8 at $z=$10.3 to 0.45 at $z=$7.4.  The noise curve (dashes) is calculated using the method described in \cite{Pober:2013p9581}  and indicates the 1$\sigma$ confidence bounds on data points consisting purely of noise; 65\% of uncorrelated noise like data points will lie below the curve. However, note the caveats that due to the weighting of the delay transform, adjacent $k$ bins are 50\% correlated. It is also clear that the sensitivity calculations under-estimate the sensitivity of the measurements by a small factor. See \cite{Pober:2014p10390,Pober:2013p9581} for a discussion of the approximations made in those calculations. The black line is a fiducial model at 50\% ionization \citep{Lidz:2008p8251}.  GMRT points from \cite{Paciga:2013p9943} indicated with 'x's and MWA points (also using 32 antenna) are black diamonds.}
%
%\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{figures/PAPER_32T_new_redshifts.png}
\caption{ \label{fig:pspecs} PAPER power spectra at four redshifts. On the top, $P_k$ spectra (a simple units change from the raw delay spectrum) provide a useful diagnostic on foreground rejection, while on the bottom we plot in $\Delta^2$ cosmological units, error bars are 2$\sigma$.    Redshift increases left to right. All spectra have an effective bandwidth of $\pm$10MHz  covering the redshift span $\Delta z\approx$1421MHz ($B/f^2$) which ranges from 0.8 at $z=$10.3 to 0.45 at $z=$7.4.  The noise curve (dashes) is calculated using the method described in \cite{Pober:2013p9581}  and indicates the 1$\sigma$ confidence bounds on data points consisting purely of noise; 65\% of uncorrelated noise like data points will lie below the curve. However, note the caveat that due to the weighting of the delay transform, adjacent $k$ bins are 50\% correlated.  See \cite{Pober:2014p10390,Pober:2013p9581} for a discussion of the approximations made in those calculations. The black line is a fiducial model at 50\% ionization \citep{Lidz:2008p8251}.  GMRT points from \cite{Paciga:2013p9943} indicated with 'x's and MWA points (also using 32 antenna) are black diamonds.   }
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{{figures/pspec_log_z_7.68}.png}
\caption{\label{fig:P14compare} The redshift 7.68 bin has been reprocessed for comparison with the P14 result.  See figure \ref{fig:pspecs} for a detailed description of curves. Compare with figure 6 of P14. Note that the LST range processed here is 3 hours less than P14 for a small reduction in sensitivity but large reduction in foreground. }
\end{figure}


\section{Results}
\label{sec:results}
\subsection{Foreground Filtering and Noise Levels}
\label{sec:noise}
The root-mean-square of the filtered visibilities measures how well we've removed foregrounds.  In Figure \ref{fig:noise} we examine $T_{rms}$ as calculated by differencing between adjacent channels and between redundant baselines. We see that in the last stage of coherent integration, the step between 40 second to 789 second integrations, the noise level decreases by the expected factor of $\sqrt{789/40}$.  At both stages the noise level found by both methods is roughly consistent with the theoretical level, only deviating significantly at the edges where the effective signal to noise due to the decreasing passband gain, Blackman Harris window, and uneven sampling (see Fig \ref{fig:bins}) drops precipitously. 

The theoretical visibility r.m.s. noise level is a straightforward calculation, but given the method employed here, should be given due description.
The noise level in a long integration (Eq. \ref{eq:Trms}) is caused by the temperature of the sky (mostly due to galactic emission) which varies with LST and by the receiver noise, both on the scale of a few 100K. The noise of any long integration is also dependent on the number of observations in each LST.  The number of points in each bin is mostly determined by the observing schedule, which for PAPER is sunrise to sunset, but also by any flagging of data or observing outage (only 92 of the 103 days in the observing period have recorded data).  Unfortunately, technical problems limited the retention of the exact cumulative count of points averaged into each LST bin during the averaging process.  Here we have chosen to use the first order estimate based on the observing schedule to estimate the number of observations in each LST. Some bins are observed only a few times, while a slim range in the LST range 6 - 9 hours is observed on every night.  Over a number of LST bins $N_\textrm{lst}$ the average effective time per bin $N_{eff}$ is given by 

%\[
%\frac{1}{\sqrt{N_{eff}}} = \frac{1}{N_{\textrm{lsts}}} \sum_\textrm{lst=1}^\textrm{lst=9}{\frac{1}{\sqrt{N(lst)}}}
%\]
\[
\frac{1}{\sqrt{N_{eff}}}  = \left\langle \frac{1}{\sqrt{N(lst)}}  \right\rangle_{\textrm{lst}}
\]
On average each bin is in this dataset is effectively measured 39.6 times. Given this accounting of the number of samples $N(lst)$, we estimate the r.m.s. temperature of the averaged data set to be

%\[
%T_{rms}  =\frac{1}{N_{\textrm{lsts}}}\sum_{\textrm{lst}} \frac{T_{sky}(lst) + T_{rcvr}}{\sqrt{2BtN(lst)}} \approx \frac{1}{\sqrt{2BtN_{eff}}} \left(\frac{1}{N_{\textrm{lsts}}}\sum_{\textrm{lst}}T_{sky}(lst) + T_{rcvr}\right)
%\label{eq:Trms}
%\]

\[
T_{rms}  =\frac{1}{N_{\textrm{lsts}}}\sum_{\textrm{lst}} \frac{T_{sky}(lst) + T_{rcvr}}{\sqrt{2BtN(lst)}} \approx \frac{(<T_{sky}> + T_{rcvr})}{\sqrt{2BtN_{eff}}}
\label{eq:Trms}
\]


\noindent{where B and t are the 496kHz channel width and lst bin length, respectively and the approximation indicates that we have assumed average values for the number of samples and the time dependent sky component of the system temperature.  In P14 the total system temperature ($T_{sky} + T_{rcvr}$) was estimated to be 550K using 20 nights of the 92 night set which covered LSTs 3-9. Using the global sky model  \citep{deOliveiraCosta:2008p2242} and a model of the PAPER primary beam we estimate the mean $T_{sky}$ during that 20 night period to be 250K, implying a receiver temperature of 300K.  The mean temperature over the full 92 night period is only 4K higher; the estimate of 550K for the system temperature is a good estimate for the full observing period.  $T_{rms}$ using 550K and an effective sample count of 39.6  is plotted as a dotted line in Figure \ref{fig:noise} for before and after fringe rate filtering (t=40s and 789s)}


The final fringe-rate step is the last coherent average of the processing routine and represents the last large gain in sensitivity; all following averaging steps are done incoherently on the square of the visibilities where sensitivity is gained at a much slower rate. For this reason the noise curves (bottom three curves of Figure \ref{fig:noise})  merit closer inspection.  At this sensitivity level we see a notable excess of power in the r.m.s. difference between baselines. Though the cause of this excess is unknown, it is suggestive of the excess covariance between redundant baselines which we remove in Section \ref{sec:power_spectrum} a hypothesis also supported by the high levels of residual foregrounds in the redshift 10.5 bin where the baseline difference r.m.s. is highest.




\subsection{Power Spectra}
\label{sec:pspecs}

In  Figure \ref{fig:pspecs} we show the  power spectra at different redshift bins --showing both the spherically averaged power spectrum $P(k)$ and the volume independent  $\Delta^2(k)\equiv\frac{k^3}{2\pi^2}P(k)$.  The $P(k)$ spectrum averages the three baseline types but preserves the positive and negative delays.  In figure \ref{fig:slices} we see different $k$ mode slices as a function of redshift,  all plotted with $2\sigma$ error bars and in Table \ref{tab:data} we list the data plotted in these slices.   

The theoretical noise power spectrum (dashed line in figures \ref{fig:pspecs} and \ref{fig:P14compare} and grey region in figure \ref{fig:slices}) is estimated using the method described in \cite{Pober:2013p9581}, assuming a system temperature of 550K and the observing scheme described in Section \ref{sec:observations}. In all figures the noise levels are plotted for comparison with the power spectrum values rather than the error bars, i.e. 65\% of uncorrelated points should lie below the line or within the grey region.    In general the theoretical error bars are larger than the bootstrap errors.  This is probably most clear in \ref{fig:slices} where, though most points are inside the error region, the bootstrap errors are only consistent with zero 25\% of the time. In general it would appear that the sensitivity calculations under-estimate the sensitivity of the measurements by a small factor. See \cite{Pober:2014p10390,Pober:2013p9581} for a discussion of the approximations made in those calculations. 



Generally, both figures convey a picture of a power spectrum dominated by noise, with foregrounds encroaching from inside and just outside the horizon to higher $k$ ``reionization'' modes.  One easy way to see this leakage is by comparing the bootstrap error bar, the theoretical error and the power spectrum value.  At the level of noise here, we expect all points outside the horizon to be consistent with zero. As a simple sorting criteria we divide the points into ``Detections'' and ``Upper Limits'' (Det/ULim) in Table \ref{tab:data} for both theoretical and bootstrap error bar.  As we can see, the larger theoretical error bars are mostly consistent with zero, while the same is true of a little less then half of the bootstrap error bars.  This is consistent with a picture of systematics or foreground leakage near the thermal noise limit.


Foreground dominance appears to grow with increasing redshift, mirroring the rise in baseline-to-baseline r.m.s. we found in Figure \ref{fig:noise}.  Recall that the wide-band foreground model we have subtracted  --which causes the notch of missing power\footnote{Note that this notch is not present in figure \ref{fig:P14compare} because the foregrounds have been left in for comparison with P14.} near $k_\parallel=0$ in Figure \ref{fig:pspecs}-- is the same for all redshift slices and was built by weighting the entire band by a Blackman-Harris window function.    Using this window, contributions to the foreground model from 126MHz ($z=$10.3) are down-weighted $\sim$75\% compared to 150Mhz ($z=$8.47). If the 126MHz sky or systematics are much different then 150Mhz foreground residuals at that redshift will be higher. The $k=0.1$hMpc$^{-1}$ slice (upper left of figure \ref{fig:slices}) is only a factor of $\sim$three away from the light travel horizon of $k=0.035$ and therefore most likely to be contaminated by low $k$ foregrounds. This $k$ mode is well above the noise and has a redshift  dependence  much the same shape as the $T_{rms}$ curve in Figure \ref{fig:noise}, with a minimum near redshift 8.5 and a dramatic rise above redshift 10.

\begin{figure}
\centering
\includegraphics[width=0.48\textwidth]{{figures/psa32_pspec_k_0.10_lin}.png}
\includegraphics[width=0.48\textwidth]{{figures/psa32_pspec_k_0.20_lin}.png}
\includegraphics[width=0.48\textwidth]{{figures/psa32_pspec_k_0.30_lin}.png}
\includegraphics[width=0.48\textwidth]{{figures/psa32_pspec_k_0.40_lin}.png}
\caption{Power spectrum amplitude vs redshift at a selection of k modes. Left to right from top, k=0.1,0.2,0.3,0.4 hMpc$^{-1}$.  Parsons 2014 PAPER limit marked with thin black, this work marked with thick blue diamonds.  The k=0.1 hMpc$^{-1}$ bin (top-left), which samples the delay spectrum at only 2x the maximum horizon delay  is foreground dominated with  a redshift dependence similar to the  $T_{rms}$ residual in Figure \ref{fig:noise}.  The rest have amplitudes below the predicted noise level, but are not themselves perfectly consistent with noise, particularly at high redshifts. Though the points are all within the region predicted to be dominated by noise (grey) the variation between the redshifts and size of error bars suggests that a small residual signal is probably still present.  The theoretical noise estimate is likely to be off by a small amount (see figure \ref{fig:pspecs} and Section \ref{sec:pspecs}).  \label{fig:slices}}
\end{figure}

\begin{deluxetable}{llrrrrr}
\tablecolumns{7}
\tablecaption{Power spectrum values}
\tablehead{
\colhead{k} & \colhead{redshift}  & \colhead{$\Delta^2$} &
\colhead{bootstrap error } &\colhead{\parbox{6em}{noise model error} }& \colhead{\parbox{5em}{bootstrap significance}} & \colhead{\parbox{6em}{noise model significance}}\\

\colhead{[hMpc$^{-1}$]} & \colhead{}  & \colhead{[mK$^2$]} &
\colhead{ [mK$^2$]} &\colhead{\parbox{7em}{[mK$^2$]} }& \colhead{} & \colhead{}
}
\startdata
0.1 & 10.29 & 43575.5512 & $\pm$ 5729.3347 & $\pm$ 8029.1131 & Det & Det \tabularnewline
0.1 & 8.54 & 5701.6267 & $\pm$ 1168.7025 & $\pm$ 3671.887 & Det & Det \tabularnewline
0.1 & 7.94 & 7695.373 & $\pm$ 2044.5824 & $\pm$ 3133.8433 & Det & Det \tabularnewline
0.1 & 7.55 & 9461.8178 & $\pm$ 2761.5645 & $\pm$ 2393.1972 & Det & Det \tabularnewline
0.2 & 10.29 & 11410.9746 & $\pm$ 5039.8271 & $\pm$ 27281.158 & Det & ULim \tabularnewline
0.2 & 8.54 & 5855.8909 & $\pm$ 2195.8363 & $\pm$ 12275.7791 & Det & ULim \tabularnewline
0.2 & 7.94 & 5143.6577 & $\pm$ 1799.7316 & $\pm$ 10221.3583 & Det & ULim \tabularnewline
0.2 & 7.55 & 856.316 & $\pm$ 1490.2202 & $\pm$ 7636.2256 & ULim & ULim \tabularnewline
0.3 & 10.29 & 34756.4662 & $\pm$ 16384.7719 & $\pm$ 64977.9446 & Det & ULim \tabularnewline
0.3 & 8.54 & 12923.2987 & $\pm$ 5373.4928 & $\pm$ 29058.6627 & Det & ULim \tabularnewline
0.3 & 7.94 & 5204.3757 & $\pm$ 4834.4927 & $\pm$ 24046.4004 & ULim & ULim \tabularnewline
0.3 & 7.55 & 7628.4564 & $\pm$ 4810.3274 & $\pm$ 17762.1184 & Det & ULim \tabularnewline
0.4 & 10.29 & 42909.9827 & $\pm$ 30695.0274 & $\pm$ 128355.9988 & ULim & ULim \tabularnewline
0.4 & 8.54 & 24115.248 & $\pm$ 16585.6317 & $\pm$ 56571.7524 & Det & ULim \tabularnewline
0.4 & 7.94 & 14730.7401 & $\pm$ 9930.2065 & $\pm$ 46725.4956 & Det & ULim \tabularnewline
0.4 & 7.55 & -8601.8091 & $\pm$ 14191.4889 & $\pm$ 34339.7156 & ULim & ULim \tabularnewline
\enddata
\tablenotetext{a}{Det indicates a measurement and error-bar inconsistent with zero, ULim indicates consistency with zero at 2 $\sigma$.}
\label{tab:data}
\end{deluxetable}


\section{Conclusions}
\label{sec:conclusion}



The power of the highly redshifted 21 cm as a cosmic probe lies in its ability to probe a 3D volume by observing at different frequencies.  The present analysis extends the work of  \cite{Parsons:2014p10499}, which used a single-baseline delay spectrum analysis to place an upper limit on the 21 cm power at z = 7.68 (163 MHz).  In this work, we take advantage of the wide bandwidth of the PAPER instrument to place limits on the power spectrum at a range of redshifts $10.7>z>7.2$, demonstrating that the algorithms used in P14 can exploit the full potential of the 21 cm line.  

Most of the power spectrum data points demonstrate a removal of foreground signals to approximately the thermal noise limit, leaving systematics on the scale of the noise. Noise limited bins --those which are statistically consistent with zero and marked as ULim in Table \ref{tab:data}-- occur at several $k$ modes at redshifts 7.55, 7.94 and 10.29. The deepest point --at redshift 7.55 and $k=0.2$-- is a 2$\sigma$ upper limit of (48mK)$^2$. For comparison, the \citet{Lidz:2008p8251} fiducial model of the reionization power spectrum estimates an amplitude of (4.4mK)$^2$, for models with 50\% reionization at $z=$7.55. 


The fact that the best upper limit still comes from the z = 7.6 band does illustrate that this redshift corresponds to a somewhat special frequency for the PAPER instrument, where the combined contribution of system noise, RFI signals, and residual foregrounds (if any remain) are at a minimum.  


Further work will look to expand on this result on several fronts.  Alternative algorithms to the wide-band CLEAN could potentially better remove the delay-space covariance introduced by RFI flags in frequency and are under investigation.  Another active point of investigation is aimed at alternative windows to the Blackman-Harris used in the wide-band CLEAN (or wholly distinct algorithms) that could better remove foregrounds from the edges of the band while still limiting foreground bleed into the EoR window.  

Future work will also include observations from subsequent seasons with more antenna.  The observations reported here demonstrate a nearly noise limited integration over a full season but used only a quarter of the final design antenna count. These  32 antenna observations from 2011 were followed by a full season of 64 antennae in 2012 and 128 in 2013.  A second season of observing with the 128 antenna array is now under way. In going from  32 to 128 antenna the mK$^2$ sensitivity increases by the expected factor of 4 and by an additional factor of $\sim$2 after accounting for the substantial uv-plane redundancy \citep{Parsons:2012p9028}. 

The analysis of the system temperature presented in section 4.1 is one of the most thorough such investigations applied to the PAPER instrument.  It confirms the effectiveness of the wideband CLEAN in removing foregrounds over a wide range of the band, while illustrating weaknesses in this approach in pushing towards the edges of PAPER's frequency range.  Furthermore, after the application of the fringe rate-based time averaging, we see hints that baseline-to-baseline variations between redundantly spaced baselines is becoming the dominant systematic in the analysis.  Techniques like the covariance removal introduced in P14 and also used in this work can differentiate some of this variation from the sky signal; however some combination of improved calibration techniques \citep{Zheng:2014p10467}, model building \citep{Sullivan:2012p9457}, and a better understanding of error covariance  \citep{Liu:2011p8763} may also prove valuable in reducing this systematic.





%(iii) third paragraph would then be on lowest freq bin and methods for improvement in analysis






%With only 32 antenna, this data lacks the sensitivity to exclude all but the most extreme models, however, it does demonstrate the ability of the wideband filtration method to affectively remove foregrounds to the precision needed to integrate a full season to the thermal noise limit.  As future observations with PAPER  will add sensitivity primarily by increasing to 128 antennae this projects well for our ability to reach design sensitivity.  The residual foregrounds analyzed here are all at or near the level of noise and expose the next level of removal required to access a wider redshift range. 





%\begin{align}
%\label{eq:BrightnessFluctuations}
% \Delta T &\approx \frac{T_S - T_{CMB}}{1 +z} \tau_{21} \nonumber\\
% &\approx T_0(z) \left( \frac{T_S - T_{CMB}}{T_S} \right) x_{HI} (1 + \delta) 
%\end{align}
%where $T_S$ is the 21cm line spin temperature, $x_{HI}$ is the local ionization fraction and $\delta$ is the mass over-density. All, save $T_0$, are position dependent.  $T_0$ encodes the global temperature evolution due to cosmological expansion
%\begin{align}
% T_0(z) & = 23~{\rm mK}~\left( \frac{\Omega_b h^2}{0.02} \right) \left[ \left(
% \frac{0.15}{\Omega_m h^2} \right) \left( \frac{1+z}{10} \right) \right]^{1/2}\nonumber\\
% & = 25~{\rm mK}~\left( \frac{1+z}{10} \right)^{1/2}
%\label{eq:Prefactor}
%\end{align}
%\citep[see, e.g., ][]{zaldarriaga_et_al2004,furlanetto_et_al2006}, where we have used the
%Planck 2013 parameters \citep{planck_et_al2013}.  




\bibliography{library}

\end{document}
